{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a0c179a",
   "metadata": {},
   "source": [
    "# Data synthesis pipeline\n",
    "Following the steps in this Notebook will allow you to synthesize data as .csv input.<br>\n",
    "All names under chapter <b>2. Variables</b> should be checked and changed if necessary.<br>\n",
    "Processing and synthesizing the data may take a while depending on the chosen dataset.<br>\n",
    "Results are stored in the \\Results folder of this pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec34c39",
   "metadata": {},
   "source": [
    "## 1. Environment\n",
    "Install required python package via pip (shapely, geopandas). Warnings may be ignored. If you encounter errors, try installing the packages via Anconda Prompt (from the Start menu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59e2a8aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dd9198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install synthgauge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a4a8d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pyreadstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c98ec64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install geopandas --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582d8672",
   "metadata": {},
   "source": [
    "### 1.1 Restart Kernel or comment out .plot_functions\n",
    "To continue with the next section, restart the kernel or comment out the line: \"from Pipeline.plotting.plot_functions import map_plotter, gemeente_lader, distribution_comparison\" (note that you won't be able to execute chapter 5.4). To restart the kernel, go to \"Kernel\" in the toolbar and select \"Restart\". After restart you can execute the following block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cced314c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T10:44:20.736025Z",
     "start_time": "2023-01-30T10:43:55.192897Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import all the required modules. Do not change these settings.\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "#import Pipeline.final_score\n",
    "\n",
    "from Pipeline.full_data_process import full_data_process\n",
    "from Pipeline.df_compare import *\n",
    "from Pipeline.privacy_functions import privacy_calc, privacy_calc_id\n",
    "from Pipeline.final_score import final_scoring\n",
    "from Pipeline.plotting.plot_functions import map_plotter, gemeente_lader, distribution_comparison\n",
    "from Pipeline.data_processing.df_comparison.df_comparer import correlation_comparison\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60801809",
   "metadata": {},
   "source": [
    "## 2. Variables\n",
    "The following section contains the variables and configuration used for the data synthesization. You can change these accordingly, depending on where the (input) data is located and where you would like to store the output (synthethic data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b57c497",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T10:44:20.743025Z",
     "start_time": "2023-01-30T10:44:20.738024Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#dataused = 'utrecht_housing'\n",
    "dataused = 'fake_nutrition'\n",
    "#dataused = 'daily_food_nutrition_nl'\n",
    "generate_new_postcodes = False\n",
    "\n",
    "# Columns to drop from the dataframe (default: empty)\n",
    "drop_cols = []\n",
    "\n",
    "# The name used for the synthetic files and subfolders\n",
    "name = dataused\n",
    "# The location where all the output such as the training dataset and synthetic dataset will be stored.\n",
    "output_location = os.path.join(os.getcwd(), 'Results\\\\' + name)\n",
    "\n",
    "\n",
    "if dataused == 'utrecht_housing':\n",
    "    \n",
    "    # The full data location of the dataset that will be used as input\n",
    "    full_data_location = 'input_test/utrechthousinghuge.csv' # example dataset with real NL zip codes and skewed age distribution following the NL population\n",
    "    # Y columns are the columns in the dataset for which the utility score will be calculated (default: age, gender). \n",
    "    y_columns = ['retailvalue', 'askingprice', 'energylabel']\n",
    "    # ID columns are the columns in the dataset for which additional privacy scores will be calculated (default: age, gender, zip_code)\n",
    "    id_columns = ['id']\n",
    "    # Differential Privacy columns\n",
    "    dp_columns = ['xcoor', 'ycoor']\n",
    "    # Variables used for drawing the graphs, col is the column which will be plotted for both the map and the distribution\n",
    "    col = 'retailvalue'\n",
    "    # The name of the column containing the zip_code4 data (default: zip_code)\n",
    "    zip_code_column = 'zipcode'\n",
    "\n",
    "elif dataused == 'fake_nutrition':\n",
    "    # The full data location of the dataset that will be used as input\n",
    "    full_data_location = 'input_test/fake_nutrition_nl.csv' # example dataset with real NL zip codes and skewed age distribution following the NL population\n",
    "    # Y columns are the columns in the dataset for which the utility score will be calculated (default: age, gender). \n",
    "    y_columns = ['daily_kcal', 'proteins_g', 'fruit_portions', 'gender']\n",
    "    # ID columns are the columns in the dataset for which additional privacy scores will be calculated (default: age, gender, zip_code)\n",
    "    id_columns = ['age', 'gender', 'zip_code'] \n",
    "    # Differential Privacy columns\n",
    "    dp_columns = ['age', 'zip_code'] \n",
    "    # Variables used for drawing the graphs, col is the column which will be plotted for both the map and the distribution\n",
    "    col = 'daily_kcal'\n",
    "    # The name of the column containing the zip_code4 data (default: zip_code)\n",
    "    zip_code_column = 'zip_code'\n",
    "\n",
    "elif dataused == 'daily_food_nutrition_nl':\n",
    "    # The full data location of the dataset that will be used as input\n",
    "    full_data_location = 'input_test/daily_food_nutrition_with_demo - kopie.csv' # original testing file with random zip codes (not real NL)\n",
    "    # Y columns are the columns in the dataset for which the utility score will be calculated (default: age, gender). \n",
    "    y_columns = ['age','gender']\n",
    "    # ID columns are the columns in the dataset for which additional privacy scores will be calculated (default: age, gender, zip_code)\n",
    "    id_columns = ['age', 'gender', 'zip_code']\n",
    "    # Differential Privacy columns\n",
    "    dp_columns = ['age', 'zip_code']\n",
    "    # Variables used for drawing the graphs, col is the column which will be plotted for both the map and the distribution\n",
    "    col = 'Calories (kcal)'\n",
    "    # The name of the column containing the zip_code4 data (default: zip_code)\n",
    "    zip_code_column = 'zip_code'\n",
    "\n",
    "else:\n",
    "    print('Select a valid dataset in the variable dataused')\n",
    "          \n",
    "\n",
    "# The location of the synthpop file\n",
    "synthpop_file = os.path.join(os.getcwd(), 'R_scripts\\synthpop_script.R')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98e7202",
   "metadata": {},
   "source": [
    "For testing only, using the daily_food_nutrition_with_demo.csv, this functions reverts the simulated zip codes to real zip codes in the Netherlands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fdbe5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Optional: broad built-in NL pool (4-digit prefixes spread across NL; not exhaustive)\n",
    "# Tip: pass your own complete list via nl_prefixes or nl_prefixes_csv for 100% coverage.\n",
    "NL_PREFIXES_DEFAULT = [\n",
    "    # Amsterdam / Amstelland\n",
    "    *[str(x) for x in range(1000, 1100)],  # 1000–1099\n",
    "    \"1111\",\"1112\",\"1113\",\"1114\",\"1115\",\"1117\",\"1118\",  # Schiphol / Haarlemmermeer\n",
    "    # Haarlem / IJmond\n",
    "    *[str(x) for x in range(2000, 2030)], \"2061\",\"2063\",\"2071\",\"2082\",\n",
    "    # Den Haag / Delft / Westland\n",
    "    *[str(x) for x in range(2500, 2600)], \"2611\",\"2612\",\"2624\",\"2631\",\"2678\",\n",
    "    # Rotterdam / Rijnmond\n",
    "    *[str(x) for x in range(3000, 3080)], \"3111\",\"3131\",\"3141\",\"3201\",\"3223\",\n",
    "    # Utrecht / Amersfoort\n",
    "    *[str(x) for x in range(3400, 3420)], *[str(x) for x in range(3500, 3600)], \"3701\",\"3811\",\"3821\",\n",
    "    # ’s-Hertogenbosch / Eindhoven / Tilburg / Breda\n",
    "    *[str(x) for x in range(4800, 4820)], \"4901\",\"5011\",\"5038\",\"5211\",\"5611\",\"5651\",\n",
    "    # Nijmegen / Arnhem\n",
    "    \"6511\",\"6525\",\"6531\",\"6541\",\"6811\",\"6828\",\"6841\",\n",
    "    # Zwolle / Deventer\n",
    "    \"7411\",\"7423\",\"8011\",\"8021\",\n",
    "    # Groningen (stad + provincie)\n",
    "    \"9700\",\"9711\",\"9721\",\"9731\",\"9743\",\"9751\",\"9791\",\"9931\",\"9901\",\"9991\",\n",
    "    # Friesland (Leeuwarden, Drachten, Heerenveen)\n",
    "    \"8911\",\"8931\",\"8441\",\"8448\",\"8449\",\"8442\",\"8443\",\"8461\",\n",
    "    # Drenthe (Assen, Emmen)\n",
    "    \"9401\",\"9402\",\"9405\",\"7811\",\"7821\",\n",
    "    # Overijssel (Enschede, Hengelo)\n",
    "    \"7511\",\"7522\",\"7551\",\"7607\",\n",
    "    # Zeeland\n",
    "    \"4331\",\"4381\",\"4461\",\"4531\",\"4561\",\"4382\",\n",
    "    # Limburg (Maastricht, Heerlen, Venlo)\n",
    "    \"6211\",\"6221\",\"6411\",\"6412\",\"5911\",\"5921\",\n",
    "    # Flevoland (Lelystad, Almere)\n",
    "    \"1311\",\"1321\",\"1331\",\"1341\",\"1351\",\"8211\",\"8221\",\"8232\"\n",
    "]\n",
    "\n",
    "def force_zip_pool_netherlands(\n",
    "    df: pd.DataFrame,\n",
    "    col: str = \"zip_code\",\n",
    "    seed: int = 42,\n",
    "    nl_prefixes: list[str] | None = None,\n",
    "    nl_prefixes_csv: str | None = None,\n",
    "    csv_column: str = \"postcode4\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Overwrite df[col] with randomized 4-digit Dutch postcodes (no letters).\n",
    "    Keeps the column as categorical (pandas 'category').\n",
    "\n",
    "    Priority:\n",
    "      1) If nl_prefixes list is provided, sample from it.\n",
    "      2) Else if nl_prefixes_csv is provided, read csv_column and use those 4-digit codes.\n",
    "      3) Else use a built-in broad NL pool (not exhaustive).\n",
    "\n",
    "    Notes:\n",
    "      - Expects 4-digit strings like '1234'. If your CSV has '1234 AB', pre-clean or we will extract digits.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = len(df)\n",
    "\n",
    "    # choose pool\n",
    "    pool = None\n",
    "    if nl_prefixes:\n",
    "        pool = pd.Series(nl_prefixes, dtype=\"string\")\n",
    "    elif nl_prefixes_csv:\n",
    "        try:\n",
    "            pool = pd.read_csv(nl_prefixes_csv)[csv_column].astype(\"string\")\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] Could not read '{nl_prefixes_csv}': {e}. Falling back to default NL pool.\")\n",
    "\n",
    "    if pool is not None:\n",
    "        # normalize to 4-digit strings: strip spaces/letters, keep first 4 digits if present\n",
    "        pool = (pool.str.extract(r\"(\\d{4})\", expand=False)\n",
    "                     .dropna()\n",
    "                     .astype(str)\n",
    "                     .str.zfill(4))\n",
    "        pool = pool[pool.str.match(r\"^\\d{4}$\")]\n",
    "        if len(pool) == 0:\n",
    "            print(\"[warn] Provided pool had no usable 4-digit NL codes. Falling back to default NL pool.\")\n",
    "            pool = pd.Series(NL_PREFIXES_DEFAULT, dtype=\"string\")\n",
    "    else:\n",
    "        pool = pd.Series(NL_PREFIXES_DEFAULT, dtype=\"string\")\n",
    "\n",
    "    # sample and assign\n",
    "    df[col] = pd.Series(rng.choice(pool.values, size=n), index=df.index)\n",
    "\n",
    "    # ensure categorical\n",
    "    df[col] = df[col].astype(\"category\")\n",
    "    return df\n",
    "\n",
    "\n",
    "if dataused == 'daily_food_nutrition_nl' and generate_new_postcodes:\n",
    "    df = pd.read_csv(full_data_location)\n",
    "\n",
    "    # Option A: use built-in broad NL pool\n",
    "    df = force_zip_pool_netherlands(df, col=\"zip_code\")\n",
    "\n",
    "    # Option B (recommended): pass a complete list or CSV of valid NL 4-digit postcodes\n",
    "    # df = force_zip_pool_netherlands(df, col=\"zip_code\", nl_prefixes=my_full_list)\n",
    "    # df = force_zip_pool_netherlands(df, col=\"zip_code\", nl_prefixes_csv=\"nl_postcode4.csv\", csv_column=\"postcode4\")\n",
    "\n",
    "    df.to_csv(full_data_location, index=False)  # overwrite\n",
    "    print('New postcodes generated and saved in the csv file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f9dae6",
   "metadata": {},
   "source": [
    "## 3. Data processing\n",
    "The following section performs the necessary data (pre-)processing steps. The output folders will be created, including the training and holdout datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42f55c37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T10:44:20.752031Z",
     "start_time": "2023-01-30T10:44:20.745024Z"
    }
   },
   "outputs": [],
   "source": [
    "synth_csv_folder = Path(output_location) / f\"{name}_synths\"\n",
    "synth_csv_folder.mkdir(parents=True, exist_ok=True)  # cria a pasta se não existir\n",
    "\n",
    "train_csv_location   = str(Path(output_location) / f\"{name}_train.csv\")\n",
    "holdout_csv_location = str(Path(output_location) / f\"{name}_holdout.csv\")\n",
    "synth_csv_location   = str(synth_csv_folder / f\"{name}_synthpop.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42caa75d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T10:44:20.772021Z",
     "start_time": "2023-01-30T10:44:20.754025Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the folder of the output location (only if it does not exist yet)\n",
    "if not os.path.exists(output_location):\n",
    "    os.makedirs(output_location)\n",
    "# Create the folder of for the synthetic datasets (only if it does not exist yet)\n",
    "if not os.path.exists(synth_csv_folder):\n",
    "    os.makedirs(synth_csv_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c592b885",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T10:44:47.373156Z",
     "start_time": "2023-01-30T10:44:20.774026Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Amount of binary columns:  0 \n",
      " Amount of categorical columns:  1 \n",
      " Amount of continues columns:  7\n",
      "Finished processing\n"
     ]
    }
   ],
   "source": [
    "# This function will make sure the file can be processed by synthpop\n",
    "# The function will print the column types: binary, categorical and continues columns\n",
    "full_data_process(file_loc=full_data_location, train_test_path=output_location, name=name, drop_cols=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc29667f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PinhoUlianA\\OneDrive - UMCG\\Documenten\\GitHub\\my_synthetic-data-pipeline\\R_scripts\\synthpop_script.R\n"
     ]
    }
   ],
   "source": [
    "# This script uses the provided (local) R-libraries. \n",
    "# Verify if the script accesses the correct folder containing the synthpop package.\n",
    "print(synthpop_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be091bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes shure the script finds R in the right location\n",
    "rscript_loc = os.getenv(\"RSCRIPT_PATH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca05f8e",
   "metadata": {},
   "source": [
    "# 4. Running Synthpop\n",
    "The code below will run the R script and use synthpop to create synthetic data <br>\n",
    "The script may take a while to run. <br>\n",
    "<br>\n",
    "Synthpop default processing method: <b>Visit order of empty columns first.</b> The order can be altered in 'R_scripts\\synthpop_script.R'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afea5dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(os.environ.get(\"RSCRIPT_PATH\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7286b188",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T10:46:49.830769Z",
     "start_time": "2023-01-30T10:44:47.375158Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# runs the Synthpop script indicating the dataused, which will motivate different types of analysis depending on the dataset. \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# These analysis are specified in the synthpop_script.R\u001b[39;00m\n\u001b[0;32m      3\u001b[0m rscript_loc \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRSCRIPT_PATH\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m process \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mrscript_loc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynthpop_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_csv_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynth_csv_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzip_code_column\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataused\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# print the output and errors (if any)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m process\u001b[38;5;241m.\u001b[39mstdout:\n",
      "File \u001b[1;32mc:\\Users\\PinhoUlianA\\AppData\\Local\\anaconda3\\envs\\sdp\\lib\\subprocess.py:971\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n\u001b[0;32m    967\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[0;32m    968\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m    969\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m--> 971\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[0;32m    982\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[1;32mc:\\Users\\PinhoUlianA\\AppData\\Local\\anaconda3\\envs\\sdp\\lib\\subprocess.py:1380\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1378\u001b[0m     args \u001b[38;5;241m=\u001b[39m list2cmdline([args])\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1380\u001b[0m     args \u001b[38;5;241m=\u001b[39m \u001b[43mlist2cmdline\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m executable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1383\u001b[0m     executable \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfsdecode(executable)\n",
      "File \u001b[1;32mc:\\Users\\PinhoUlianA\\AppData\\Local\\anaconda3\\envs\\sdp\\lib\\subprocess.py:563\u001b[0m, in \u001b[0;36mlist2cmdline\u001b[1;34m(seq)\u001b[0m\n\u001b[0;32m    561\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    562\u001b[0m needquote \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 563\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(os\u001b[38;5;241m.\u001b[39mfsdecode, seq):\n\u001b[0;32m    564\u001b[0m     bs_buf \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    566\u001b[0m     \u001b[38;5;66;03m# Add a space to separate this argument from the others\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PinhoUlianA\\AppData\\Local\\anaconda3\\envs\\sdp\\lib\\os.py:823\u001b[0m, in \u001b[0;36m_fscodec.<locals>.fsdecode\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfsdecode\u001b[39m(filename):\n\u001b[0;32m    818\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Decode filename (an os.PathLike, bytes, or str) from the filesystem\u001b[39;00m\n\u001b[0;32m    819\u001b[0m \u001b[38;5;124;03m    encoding with 'surrogateescape' error handler, return str unchanged. On\u001b[39;00m\n\u001b[0;32m    820\u001b[0m \u001b[38;5;124;03m    Windows, use 'strict' error handler if the file system encoding is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;124;03m    'mbcs' (which is the default encoding).\u001b[39;00m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 823\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Does type-checking of `filename`.\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filename, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m    825\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mdecode(encoding, errors)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "# runs the Synthpop script indicating the dataused, which will motivate different types of analysis depending on the dataset. \n",
    "# These analysis are specified in the synthpop_script.R\n",
    "rscript_loc = os.environ.get(\"RSCRIPT_PATH\")\n",
    "process = subprocess.Popen(\n",
    "    [rscript_loc, synthpop_file, train_csv_location, synth_csv_location, zip_code_column, dataused],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "# print the output and errors (if any)\n",
    "for line in process.stdout:\n",
    "    print(line, end=\"\")\n",
    "\n",
    "stderr = process.stderr.read()\n",
    "if stderr:\n",
    "    print(\"STDERR:\\n\", stderr)\n",
    "\n",
    "process.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a2d794",
   "metadata": {},
   "source": [
    "# 6. Apply differential privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63f0629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "synthetic_data = pd.read_csv(synth_csv_location)\n",
    "\n",
    "# Set privacy parameter and select columns to privatize\n",
    "epsilon, sensitivity = 0.8, 1\n",
    "columns = dp_columns\n",
    "\n",
    "# Add Laplace noise to create noisy and synthetic datasets\n",
    "noisy_data = synthetic_data.copy()\n",
    "for column in columns:\n",
    "    if column not in noisy_data.columns:\n",
    "        print(f\"[warn] column '{column}' not found. Available columns: {list(noisy_data.columns)}\")\n",
    "    noisy_data[column] += np.random.laplace(0, scale=sensitivity/epsilon, size=len(synthetic_data)).round(0)\n",
    "\n",
    "# Write the noisy datasets to CSV files\n",
    "noisy_csv_location = synth_csv_folder / f\"{name}_noisy.csv\"\n",
    "noisy_data.to_csv(noisy_csv_location, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a145c8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select the column to compare\n",
    "column_to_compare = col\n",
    "\n",
    "# Print basic statistics of the real and synthetic datasets\n",
    "print('Real data:')\n",
    "print(synthetic_data[column_to_compare].describe())\n",
    "print('\\nSynthetic data:')\n",
    "print(noisy_data[column_to_compare].describe())\n",
    "\n",
    "# Compute the mean absolute difference between the real and synthetic datasets\n",
    "mad = np.mean(np.abs(synthetic_data[column_to_compare] - noisy_data[column_to_compare]))\n",
    "\n",
    "print('\\nMean Absolute Difference:', mad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb6c9ae",
   "metadata": {},
   "source": [
    "# 7. Apply k-anonymity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78832e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_round(x, base=5):\n",
    "    return int(base * round(float(x)/base))\n",
    "\n",
    "# Load dataset\n",
    "synthetic_data = pd.read_csv(synth_csv_location)\n",
    "synthetic_data = synthetic_data.dropna(subset = [zip_code_column])\n",
    "\n",
    "# Define the columns that contain sensitive information\n",
    "sensitive_columns = id_columns\n",
    "\n",
    "# Apply k-anonimity measures (rounding of variables)\n",
    "synthetic_data[zip_code_column] = synthetic_data[zip_code_column].apply(lambda x: custom_round(x, base=5)).astype(\"float64\")\n",
    "synthetic_data[col] = synthetic_data[col].apply(lambda x: custom_round(x, base=5)).astype(\"float64\")\n",
    "\n",
    "# Group the data by the sensitive columns and count the number of rows in each group\n",
    "group_counts = synthetic_data.groupby(sensitive_columns).size().reset_index(name='count')\n",
    "\n",
    "# Determine the minimum group size (k) for each sensitive attribute combination\n",
    "min_counts = group_counts.groupby(sensitive_columns)['count'].min().reset_index(name='min_count')\n",
    "\n",
    "# Compute the overall minimum group size (k-anonymity level) as the minimum of all the individual k values\n",
    "k_anonymity_level = min_counts['min_count'].min()\n",
    "\n",
    "# Print the k-anonymity level\n",
    "print('The dataset has a k-anonymity level of', k_anonymity_level)\n",
    "print(group_counts)\n",
    "\n",
    "# Write the k-anonymity datasets to CSV files\n",
    "kanon_csv_location = synth_csv_folder / f\"{name}_kanonymity.csv\"\n",
    "synthetic_data.to_csv(kanon_csv_location, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bfefb5",
   "metadata": {},
   "source": [
    "# 5. Evaluation\n",
    "The code below will evaluate the generated synthpop data against the holdout dataset using the metrics on utility, fidelity, and privacy. These outcomes can be used to evaluate the performance of synthetic data generation methods when compared to real data.<br>\n",
    "\n",
    "<b>Utility</b>: Utility refers to the usefulness of synthetic data for a particular task or analysis. In other words, how well does the synthetic data perform when used in place of real data? A synthetic dataset with high utility should be able to provide similar or equivalent results to those obtained using real data.<br>\n",
    "\n",
    "<b>Fidelity</b>: Fidelity refers to the degree to which the synthetic data accurately represents the real data. A synthetic dataset with high fidelity should be able to capture the key statistical properties of the real data, such as the mean, median, standard deviation, and distribution of variables.<br>\n",
    "\n",
    "<b>Privacy</b>: Privacy refers to the level of protection provided to individuals' personal information in the synthetic dataset. A synthetic dataset with high privacy should not be susceptible to re-identification attacks, meaning that it should not be possible to link an individual's identity to their personal information in the dataset.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c66228a",
   "metadata": {},
   "source": [
    "## 5.1 Fidelity & Utility calculations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87def993",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T10:46:57.143805Z",
     "start_time": "2023-01-30T10:46:49.832769Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "end_results, ratio_results, reggre, classi = df_compare(train_csv_location,\n",
    "                                                        holdout_csv_location, \n",
    "                                                        synth_csv_folder,\n",
    "                                                        c=1,\n",
    "                                                        y_columns=y_columns,\n",
    "                                                        subset=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840981dc",
   "metadata": {},
   "source": [
    "### 5.1.1 Fidelity results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b444a489",
   "metadata": {},
   "source": [
    "Fidelty evaluations compared to the real dataset.<br><br>\n",
    "    <b>dupe_numbers</b>: Number of duplicate records.<br>\n",
    "    <b>sum_%mean_diff</b>: This variable represents the sum of the percentage difference between the means of two sets of data. It can be used to quantify the degree of difference between the two sets of data.<br>\n",
    "    <b>sum_%median_diff</b>: This variable represents the sum of the percentage difference between the medians of two sets of data. It can be used to quantify the degree of difference between the two sets of data.<br>\n",
    "    <b>sum_%std_diff</b>: This variable represents the sum of the percentage difference between the standard deviations of two sets of data. It can be used to quantify the degree of difference between the two sets of data.<br>\n",
    "    <b>binary_val_count_diff</b>: This variable represents the difference in the count of binary values between two sets of data. It can be used to compare the frequency of occurrence of certain binary values between two sets of data.<br>\n",
    "    <b>correlation_norm</b>: This variable represents the normalized correlation between two sets of data. It can be used to measure the strength and direction of the linear relationship between the two sets of data.<br>\n",
    "    <b>real_or_snyth_acc</b>: This variable represents the accuracy of a machine learning model in classifying real versus synthetic data. It can be used to evaluate the performance of the model in distinguishing between real and synthetic data.<br>\n",
    "    <b>jenson_shannon</b>: This variable represents the Jensen-Shannon divergence between two probability distributions. It can be used to measure the dissimilarity between the two distributions.<br>\n",
    "    <b>total_variational_dist</b>: This variable represents the total variation distance between two probability distributions. It can be used to measure the distance between the two distributions.<br>\n",
    "    <b>wasserstein_dist</b>: This variable represents the Wasserstein distance between two probability distributions. It can be used to measure the distance between the two distributions, taking into account the underlying geometry of the space in which the distributions are defined.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813951ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T10:46:57.172805Z",
     "start_time": "2023-01-30T10:46:57.148807Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "end_results.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642b4c02",
   "metadata": {},
   "source": [
    "### 5.1.2 Fidelity ratio results\n",
    "Calculated by dividing all results from a synthetic dataset by the holdout dataset. <br>\n",
    "\n",
    "Each result (expect for dupe_numbers) should near 1.0 to compare the synthetic data to the holdout dataset. This would conclude a good statistical comparable synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570c443a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T10:46:57.190811Z",
     "start_time": "2023-01-30T10:46:57.175808Z"
    }
   },
   "outputs": [],
   "source": [
    "ratio_results.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3fa2d7",
   "metadata": {},
   "source": [
    "### 5.1.3 Utility regression results\n",
    "The scores calculated are the r2, the mse, and the max error.<br>\n",
    "\n",
    "The r2 score (also known as the coefficient of determination) is a very important metric that is used to evaluate the performance of a regression-based machine learning model. It works by measuring the amount of variance in the predictions explained by the dataset. Simply put, it is the difference between the samples in the dataset and the predictions made by the model.<br>\n",
    "\n",
    "If the value of the r squared score is 1, it means that the model is perfect and if its value is 0, it means that the model will perform badly on an unseen dataset. The result should be compared to the score of the holdout dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9d6d33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T10:46:57.207804Z",
     "start_time": "2023-01-30T10:46:57.192808Z"
    }
   },
   "outputs": [],
   "source": [
    "regression_metrics = ['r2', 'mean squared error', 'max error', 'explained variance score']\n",
    "\n",
    "for metric in regression_metrics:\n",
    "    df_metric = reggre.xs(metric, level=1, drop_level=False).sort_values(by=reggre.columns[:1][0], ascending=False)\n",
    "    print(f\"\\n=== metric: {metric} ===\")\n",
    "    display(df_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af9b670",
   "metadata": {},
   "source": [
    "### 5.1.4 Utility classification results\n",
    "Scores calculated are the accuracy, f1, recall, and precision.\n",
    "\n",
    "Accuracy is the percentage of correct classifications that a trained machine learning model achieves, i.e., the number of correct predictions divided by the total number of predictions across all classes. Accuracy of 0 means the classifier always predicts the wrong label, whereas accuracy of 1 means that it always predicts the correct label.<br>\n",
    "\n",
    "Accuracy is an indicator for under- and overfitting and the value should be comparable to the holdout dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52dc935",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T10:46:57.223807Z",
     "start_time": "2023-01-30T10:46:57.210806Z"
    }
   },
   "outputs": [],
   "source": [
    "class_metrics = ['accuracy', 'recall', 'precision', 'f1']\n",
    "\n",
    "for metric in class_metrics:\n",
    "    df_metric = classi.xs(metric, level=1, drop_level=False).sort_values(by=classi.columns[:1][0], ascending=False)\n",
    "    print(f\"\\n=== metric: {metric} ===\")\n",
    "    display(df_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd166290",
   "metadata": {},
   "source": [
    "## 5.2 Privacy calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b472a9ca",
   "metadata": {},
   "source": [
    "### 5.2.1 Calculations on entire dataset\n",
    "Calculate the privacy scores for the entire dataset, including all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0cbab9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T10:47:10.956875Z",
     "start_time": "2023-01-30T10:46:57.225806Z"
    }
   },
   "outputs": [],
   "source": [
    "privacy_results, privacy_ratio = privacy_calc(train_csv_location,\n",
    "                                              holdout_csv_location, \n",
    "                                              synth_csv_folder,\n",
    "                                              sample_per=75, \n",
    "                                              memory=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b52c31e",
   "metadata": {},
   "source": [
    "### 5.2.2 Calculations on quassi-identifiers\n",
    "Calculate the privacy scores based on the quasi-identifiers indicated in the configuration step (2) of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4c6c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "privacy_results_id, privacy_ratio_id = privacy_calc_id(train_csv_location,\n",
    "                                              holdout_csv_location, \n",
    "                                              synth_csv_folder,\n",
    "                                              id_columns,\n",
    "                                              sample_per=75, \n",
    "                                              memory=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cb6b1d",
   "metadata": {},
   "source": [
    "### 5.2.3 Privacy score results\n",
    "DCR (Distance to Closest Record) and NNDR (Nearest Neighbour Distance Ratio) are  two evaluation metrics commonly used in the field of record linkage, which is the process of identifying records in different data sources that refer to the same entity. The values for the synthetic data should be comparable to the holdout dataset. Significantly lower scores indicate that records are close to the actual data and that the model overfits. \n",
    "\n",
    "The first two results are based on the entire dataset. The last two results are based on only the quasi-identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3291ff40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T10:51:45.068249Z",
     "start_time": "2023-01-30T10:51:45.057244Z"
    }
   },
   "outputs": [],
   "source": [
    "privacy_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d44530e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T10:47:10.972875Z",
     "start_time": "2023-01-30T10:47:10.958877Z"
    }
   },
   "outputs": [],
   "source": [
    "privacy_ratio2 = privacy_ratio.T.add_suffix('_ratio')\n",
    "priv_both = pd.merge(privacy_results.T, privacy_ratio2, left_index=True, right_index=True)\n",
    "priv_both.sort_values(by='DCR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8516cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "privacy_results_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6a9438",
   "metadata": {},
   "outputs": [],
   "source": [
    "privacy_ratio2 = privacy_ratio_id.T.add_suffix('_ratio')\n",
    "priv_both = pd.merge(privacy_results_id.T, privacy_ratio2, left_index=True, right_index=True)\n",
    "priv_both.sort_values(by='DCR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1711a5",
   "metadata": {},
   "source": [
    "## 5.3 Final score\n",
    "In comparing a synthetic dataset with real data, it is important to evaluate each of these variables to ensure that the synthetic data is a suitable replacement for real data in a given analysis or task. A high level of utility and fidelity suggests that the synthetic data can be used with confidence, while a high level of privacy suggests that individuals' personal information is well protected.<br>\n",
    "\n",
    "<b>privacy</b>: Privacy score for the entire dataset<br>\n",
    "<b>privacy on ids</b>: Privacy score for the quasi-identifiers<br>\n",
    "<b>fidelity</b>: Statistical comparison between the datasets<br>\n",
    "<b>utlity</b>: Correlations between variables in the dataset<br>\n",
    "\n",
    "<b>For the final score, the lower the score per domain, the better the performance in that domain.</b><br> As explained by Rients:\n",
    "<i>For each of the three domains multiple evaluation methods have been used to assess the performance of each dataset. This results in a large number of scores, which can be difficult to interpret and draw conclusions from. To obtain a clearer understanding of the performance of each dataset, a final score has been calculated. These scores are calculated by aggregating the individual scores resulting in a clear overview of the performance of each dataset in each domain. Not every individual score contributes as much to the final aggregated score, because based on initial results certain methods such as calculating the sum percentage difference of the standard deviation returned unstable results. All aggregated scores have been calculated in a penalty like matter, meaning that the lower the score is, the better the dataset has performed.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9466e2ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T10:47:11.017873Z",
     "start_time": "2023-01-30T10:47:10.973875Z"
    }
   },
   "outputs": [],
   "source": [
    "end_score, priv_score, priv_score_id, fidel_score, ml_score, fin_frame = final_scoring(ratio_results, privacy_ratio, privacy_ratio_id, reggre, classi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0d0e0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T10:47:11.028876Z",
     "start_time": "2023-01-30T10:47:11.019874Z"
    }
   },
   "outputs": [],
   "source": [
    "end_score.sort_index(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0278c335",
   "metadata": {},
   "source": [
    "## 5.4 Graph plotting\n",
    "For visual comparison of the results various graphs and plots can be used. These are computed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd03fdd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T10:47:12.460886Z",
     "start_time": "2023-01-30T10:47:11.039874Z"
    }
   },
   "outputs": [],
   "source": [
    "gemeentes = gemeente_lader()\n",
    "df_real, synth_frame = data_loader(train_csv_location, holdout_csv_location, synth_csv_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee747d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_real.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ce0702",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T10:47:11.037876Z",
     "start_time": "2023-01-30T10:47:11.033872Z"
    }
   },
   "outputs": [],
   "source": [
    "# Change this to one of the columns to alter the graphs.\n",
    "col = col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d35c7c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T10:47:13.837888Z",
     "start_time": "2023-01-30T10:47:12.462881Z"
    }
   },
   "outputs": [],
   "source": [
    "plots = []\n",
    "for frame in synth_frame:\n",
    "    df = pd.read_csv(synth_frame[frame])\n",
    "    df.columns = df_real.columns \n",
    "    print('Mean for', col, 'in', frame, df[col].mean())\n",
    "    plots.append(map_plotter(df_real, df, gemeentes, frame, column=col, zip_code=zip_code_column))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b6f46f",
   "metadata": {},
   "source": [
    "### 5.4.1 Geographical spread and visual\n",
    "Showing averages of a column grouped on zip code/municipality. Atleast 25 participants need to have the same zip code to be included in the visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3868ef7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T10:47:14.464898Z",
     "start_time": "2023-01-30T10:47:13.839890Z"
    }
   },
   "outputs": [],
   "source": [
    "plots[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041c1830",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T10:47:15.004898Z",
     "start_time": "2023-01-30T10:47:14.466892Z"
    }
   },
   "outputs": [],
   "source": [
    "plots[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f1086d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0add16bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc422fed",
   "metadata": {},
   "source": [
    "### 5.4.2 Univariate distribution plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ff5cfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T10:47:15.011894Z",
     "start_time": "2023-01-30T10:47:15.006895Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df_real[col].min(), df_real[col].max(), df_real[col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a4bdf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T10:47:15.016896Z",
     "start_time": "2023-01-30T10:47:15.013897Z"
    }
   },
   "outputs": [],
   "source": [
    "# This variable could be changed.\n",
    "# If there are to many bars in the distribution increasing the split nr will benefit this.\n",
    "# With binary columns change it to 0.5\n",
    "split = 6\n",
    "\n",
    "# Change this to one of the columns to alter the graphs.\n",
    "col = col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1679d49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T10:47:15.366897Z",
     "start_time": "2023-01-30T10:47:15.020898Z"
    }
   },
   "outputs": [],
   "source": [
    "dist  = []\n",
    "for frame in synth_frame:\n",
    "    df = pd.read_csv(synth_frame[frame])\n",
    "    df.columns = df_real.columns \n",
    "    dist.append(distribution_comparison(df_real, df, column=col, step_split=split, name=frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7ef1c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T10:47:16.032899Z",
     "start_time": "2023-01-30T10:47:15.368896Z"
    }
   },
   "outputs": [],
   "source": [
    "dist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508e6382",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T10:47:16.693903Z",
     "start_time": "2023-01-30T10:47:16.034900Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dist[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee34d25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
